{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/z-tufekci/DeepLearning/blob/main/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5nk87blfl3j"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Dense, BatchNormalization, LeakyReLU, Reshape,Conv2DTranspose, Conv2D, Dropout, Flatten)\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPgms8_ngL2z",
        "outputId": "21ad4a66-3bfe-4537-ac18-96a3dec0b36e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import pandas as pd\n",
        "df=pd.read_csv('gdrive/My Drive/DeepLearning/94_character_TMNIST.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaNs3dXrgqvj",
        "outputId": "75a46c99-3d59-45d9-b73a-efbf8f455df9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(274093, 28, 28, 1)\n"
          ]
        }
      ],
      "source": [
        "X_prev = df.drop(columns={'names','labels'})\n",
        "y = df[['labels']]\n",
        "\n",
        "X = X_prev.values.reshape(X_prev.shape[0], 28, 28, 1).astype('float32')\n",
        "X = (X - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
        "print(X.shape)\n",
        "BUFFER_SIZE = 275000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Batch and shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8W0JPGp4-P_"
      },
      "source": [
        "✈ GENERATOR MODEL "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlEqs3H5nA6G"
      },
      "outputs": [],
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "    #use batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "    #use leakyrelu as activation function\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
        "\n",
        "    #CNN TRANSPOSE WITH 128 LAYER  5X5 KERNEL SIZE\n",
        "    model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    #CNN TRANSPOSE WITH 64 LAYER WITH 5X5 KERNEL SIZE\n",
        "    model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    # OUTPUT LAYER USE TANH ACTIVATION\n",
        "    model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTkAn-cv5H9H"
      },
      "source": [
        "⚓ CREATE RANDOM IMAGE WITH NOISE VALUE=100 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "oeHeaY5lnfZN",
        "outputId": "006b4cab-43bc-4505-c746-9d44b68b08bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8351377d90>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "generator = make_generator_model()\n",
        "\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AECRr9zC5se6"
      },
      "source": [
        "⚖ DISCRIMINATOR MODEL "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqNFa27Mn9Ts"
      },
      "outputs": [],
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    # 64 LAYER CNN With 5x5 kernel size\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                     input_shape=[28, 28, 1]))\n",
        "    #leaky relu as activation\n",
        "    model.add(layers.LeakyReLU())\n",
        "    # dropout 0.3 data\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    #128 LAYER CNN With 5x5 kernel size\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtNsXm_NoJH-",
        "outputId": "6badb413-016f-4cc3-aafe-c279805fdb29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([[-0.00282953]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "discriminator = make_discriminator_model()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhOBTG8PpVDR"
      },
      "outputs": [],
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wco8Cm7trdvP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5poO3Y0TtRAq"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "# We will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "num_examples_to_generate = 40\n",
        "noise_dim = 100\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ0Lik2Mtrhe"
      },
      "outputs": [],
      "source": [
        "# tf.function annotation causes the function to be \"compiled\" as part of the training\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "  \n",
        "    # 1 - Create a random noise to feed it into the model for the image generation\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    \n",
        "    # 2 - Generate images and calculate loss values GradientTape method records operations for automatic differentiation.\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    # 3 - Calculate gradients using loss values and model variables\n",
        "    # \"gradient\" method computes the gradient using \n",
        "    # operations recorded in context of this tape (gen_tape and disc_tape).\n",
        "    \n",
        "    # It accepts a target (e.g., gen_loss) variable and \n",
        "    # a source variable (e.g.,generator.trainable_variables)\n",
        "    # target --> a list or nested structure of Tensors or Variables to be differentiated.\n",
        "    # source --> a list or nested structure of Tensors or Variables.\n",
        "    # target will be differentiated against elements in sources.\n",
        "\n",
        "    # \"gradient\" method returns a list or nested structure of Tensors  \n",
        "    # (or IndexedSlices, or None), one for each element in sources. \n",
        "    # Returned structure is the same as the structure of sources.\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, \n",
        "                                               generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, \n",
        "                                                discriminator.trainable_variables)\n",
        "    \n",
        "    # 4 - Process  Gradients and Run the Optimizer\n",
        "    # \"apply_gradients\" method processes aggregated gradients. \n",
        "    # ex: optimizer.apply_gradients(zip(grads, vars))\n",
        "    \"\"\"\n",
        "    Example use of apply_gradients:\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n",
        "    # Processing aggregated gradients.\n",
        "    optimizer.apply_gradients(zip(grads, vars), experimental_aggregate_gradients=False)\n",
        "    \"\"\"\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni8MsVmcu50q"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from IPython import display # A command shell for interactive computing in Python.\n",
        "\n",
        "def train(dataset, epochs):\n",
        "  # A. For each epoch, do the following:\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    # 1 - For each batch of the epoch, \n",
        "    for image_batch in dataset:\n",
        "      # 1.a - run the custom \"train_step\" function we just declared above\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # 2 - Produce images for the GIF as we go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator, epoch + 1, seed)\n",
        "\n",
        "    # 3 - Save the model every 5 epochs as a checkpoint, which we will use later\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    # 4 - Print out the completed epoch no. and the time spent\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # B. Generate a final image after the training is completed\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator, epochs, seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "401M3NnavVml"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(5, 8))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(5, 8, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING  ♈"
      ],
      "metadata": {
        "id": "dR3-UaefioYA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di7ktIx6vc4T"
      },
      "outputs": [],
      "source": [
        "train(train_dataset, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDpZrGUlwlV9"
      },
      "outputs": [],
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bzgw5JVwl7g"
      },
      "outputs": [],
      "source": [
        "# PIL is a library which may open different image file formats\n",
        "import PIL \n",
        "# Display a single image using the epoch number\n",
        "def display_image(epoch_no):\n",
        "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n",
        "display_image(EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9OvRVtiCwsws"
      },
      "outputs": [],
      "source": [
        "import glob # The glob module is used for Unix style pathname pattern expansion.\n",
        "import imageio # The library that provides an easy interface to read and write a wide range of image data\n",
        "\n",
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "display.Image(open('dcgan.gif','rb').read())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOqUD0kvjuA4sqg0APsmOba",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}